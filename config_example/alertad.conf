import json
import os
from json import JSONDecodeError

from kombu import Queue  # noqa

# DEBUG = True

# DATABASE_URL = 'postgresql://postgres@127.0.0.1/monitoring?connect_timeout=10&application_name=alerta'
DATABASE_URL = 'flexiblededup://postgres@127.0.0.1/monitoring?connect_timeout=10&application_name=alerta'
DATABASE_NAME = 'monitoring'

#
# DEDUPLICATION
#
# DEFAULT_DEDUPLICATION_TYPE: both or attribute.
# If 'both', original Alerta deduplication (based on resource and event) is tried before.
# If 'attribute', original deduplication is not tried and deduplication will depend only on attribute 'deduplication'.
# Default: 'both'
# DEFAULT_DEDUPLICATION_TYPE = 'attribute'
#
# DEFAULT_DEDUPLICATION_TEMPLATE
# IF exists, render template to get deduplication value (if no deduplication attribute is received)
# DEFAULT_DEDUPLICATION_TEMPLATE = '{{ alert.attribute.deduplication | default(alert.id) }}'

#
# LOGGING CONFIGURATION
#
LOG_CONFIG_FILE = os.path.join(os.path.dirname(__file__), 'logging.yaml')
# LOG_LEVEL = 'DEBUG'
# LOG_HANDLERS = ['console']
# LOG_FORMAT = 'default'  # default, simple, verbose, json, syslog
# LOG_FORMAT = '%(asctime)s %(name)s[%(process)d]: [%(levelname)s] %(message)s [in %(pathname)s:%(lineno)d]'
# LOG_HANDLERS = ['file']
# LOG_FILE = '/var/log/alertad.log'
# LOG_MAX_BYTES = 5*1024*1024  # 5 MB
# LOG_BACKUP_COUNT = 2

#
# SECURITY
#
AUTH_REQUIRED = True
SECRET_KEY = 'the_secret_key'
ADMIN_USERS = ["alertaio-admin@datadope.io"]
API_KEY_EXPIRE_DAYS = 3650
# DEFAULT_ADMIN_ROLE = 'ops'
# ADMIN_ROLES = ['ops', 'devops', 'coolkids']
# USER_DEFAULT_SCOPES = ['read', 'write:alerts']
# CUSTOMER_VIEWS = True

#
# WEB CONFIG
#
# DEFAULT_PAGE_SIZE = 50
# HISTORY_LIMIT = 100
# HISTORY_ON_VALUE_CHANGE = True
# BULK_QUERY_LIMIT = 100000

#
# CELERY CONFIGURATION
#
CELERY_BROKER_URL = 'redis://localhost:6379/0'
CELERY_RESULT_BACKEND = CELERY_BROKER_URL
CELERY_IMPORTS = ("iometrics_alerta.plugins.bgtasks",)
CELERY_CREATE_MISSING_QUEUES = True
CELERY_DEFAULT_QUEUE = 'alert'
CELERYD_LOG_FORMAT = "[%(asctime)s: %(name)s/%(levelname)s/%(processName)s] %(message)s"  # Add logger name
CELERYD_TASK_LOG_FORMAT = \
    "[%(asctime)s: %(name)s/%(levelname)s/%(processName)s %(task_name)s/%(task_id)s]: %(message)s"  # Add logger name
BROKER_TRANSPORT_OPTIONS = {
    "queue_order_strategy": "priority",
    "sep": ":",  # Default: \x06\x16
    "priority_steps": list(range(10)),  # Default: 4 ranges
}

# Auto close background task configuration
AUTO_CLOSE_TASK_INTERVAL = 30.0  # Defaults: 60
AUTO_CLOSE_TASK_QUEUE = os.getenv('AUTO_CLOSE_TASK_QUEUE', 'autoclose')

#
# ALERTACLIENT_CONFIGURATION:
# Configuration for connecting to alerta sever from celery workers.
# Needed for periodic task for closing automatically alerts.
#
# Configuring from environment vars with name 'ALERTACLIENT__FIELD'
# where 'field' corresponds to one of the arguments for alert client:
# endpoint, key, secret, token, username, password, timeout, ssl_verify,
# ssl_cert, ssl_key, headers and debug.
# headers must be provided as a JSON list.
_alerta_client_configuration = {}

for k, v in os.environ.items():
    if k.startswith('ALERTACLIENT__'):
        field = k[len('ALERTACLIENT__'):].lower()
        if field in ('ssl_verify', 'debug'):
            v = str(v).strip()[0].lower() in ('t', 'y', 's')
        elif field == 'timeout':
            try:
                v = float(v)
            except:  # noqa
                v = None
        elif field == 'headers':
            try:
                v = json.loads(v)
            except:  # noqa
                v = None
        if v is not None:
            _alerta_client_configuration[field] = v

ALERTACLIENT_CONFIGURATION = {**dict(
    endpoint="http://localhost:8000",
    key=None,
    secret=None,
    token=None,
    username=None,
    password=None,
    timeout=10.0,
    ssl_verify=True,
    ssl_cert=None,
    ssl_key=None,
    headers=None,
    debug=False), **_alerta_client_configuration}

"""
Interval for auto close periodic task. Default 60 sec.
"""
#
# HOUSEKEEPING
# Debe ejecutarse el proceso de housekeeping con el cliente (alerta housekeeping)
# o con una conexiÃ³n al servidor GET /management/housekeeping
# El usuario asociado debe tener permiso admin:management
#
# ALERT_TIMEOUT = 86400  # Default alert timeout. Default 24 hours. 0 for no timeout
# ACK_TIMEOUT = 7200  # timeout for unack alerts in ack status (default is 7200 seconds, 0 = do not auto-unack)
# SHELVE_TIMEOUT = 7200  # timeout for unshelving alerts in shelved status (default 7200, 0 = do not auto-unshelve)
DELETE_EXPIRED_AFTER = 0  # Default 7200 (2 hours). 0 for not deleting expired
DELETE_INFO_AFTER = 0  # Default 43200 (12 hours). 0 for not deleting informational
DELETE_CLOSED_AFTER = 0  # Default to DELETE_EXPIRED_AFTER. 0 for not deleting closed -> Only for flexiblededup backend.

#
# PLUGINS CONFIGURATION
#
PLUGINS_RAISE_ON_ERROR = False
PLUGINS = ["remote_ip", "reject", "heartbeat", "blackout", "forwarder", "iom_preprocess"]
_alerters = os.getenv('ALERTERS', ['email'])
if isinstance(_alerters, str):
    try:
        _alerters = json.loads(_alerters)
    except JSONDecodeError:
        _alerters = [x.strip() for x in _alerters.split(',')]
PLUGINS.extend(_alerters)

#
# ORIGINAL BLACKOUTS PLUGIN_CONFIGURATION
#
BLACKOUT_DURATION = 7200  # 2 hours
NOTIFICATION_BLACKOUT = True
BLACKOUT_ACCEPT = ['normal', 'ok', 'cleared']

#
# ALERTERS CONFIGURATION: GLOBAL
#
# Task names for operations 'process_event' and 'process_recovery'.
# Task names will be used as:
#   - field names of alerter attribute where processing status is stored.
#   - "By operation" variable name prefix.
# ALERTERS_TASK_BY_OPERATION = {
#     'process_event': 'new',
#     'process_recovery': 'recovery'
# }

# Location of Alerters templates dir
# Default in code: templates dir in iometrics-alerta project.
# Modified to use templates dir as sibling os this config file dir
ALERTERS_TEMPLATES_LOCATION = os.path.abspath(os.path.join(os.path.dirname(__file__), '../templates'))

#
# Operations to configure: new and recovery.
# Every alerter may override this config using configuration <ALERTER_NAME>_TASKS_DEFINITION.
# Operations configuration can be received as an alert attribute (tasksDefinition)
# or an alert eventTag (TASKS_DEFINITION).
# Value must be a dictionary (o string with json dictionary format) with these keys:
#     - queue (str): Name of the queue that will process the operation
#     - priority (int): Priority for the task (0 highest priority, 9 lowest priority)
#     - retry_spec (dict): Specification of the mechanism for retries
#         - max_retries (int): Maximum number of retries
#         - exponential (bool): If False, interval between two retries is calculated exponentially
#         - interval_first (float): First interval
#         - interval_step (float): Additional interval between two retries (only used if exponential is False)
#         - interval_max (float): Maximum interval between two retries
#         - jitter (bool): If true, retry will be executed in a random instant between 0 and the calculated interval
#
# Interval calculation:
#     - If exponential is True: interval = min(interval_max, interval_first * (2 ** <retries>))
#     - If exponential is False: interval = min(interval_max, interval_first + interval_step * <retries>)
#
#     In both cases, if jitter is True, the actual interval will be a random instant between 0
#     and the interval calculated with the previous formula.
ALERTERS_DEFAULT_TASKS_DEFINITION = {
    "new": {"queue": "alert", "priority": 1, "retry_spec": {
        "max_retries": 32,
        "exponential": True,
        "interval_first": 2.0,  # First retry after 2 secs
        "interval_step": 5.0,  # Only for exponential = false
        "interval_max": 10.0 * 60,  # Max interval 10 min
        "jitter": False  # If true, random seconds between 0 and exponential calculated time
    }},
    "recovery": {"queue": "recovery", "priority": 6, "retry_spec": {
        "max_retries": 16,
        "exponential": True,
        "interval_first": 2.0,  # First retry after 2 secs
        "interval_step": 5.0,  # Only for exponential = false
        "interval_max": 10.0 * 60,  # Max interval 10 min
        "jitter": False  # If true, random seconds between 0 and exponential calculated time
    }},
    "repeat": {"queue": "repeat", "priority": 7, "retry_spec": {
        "max_retries": 5,
        "exponential": True,
        "interval_first": 2.0,  # First retry after 2 secs
        "interval_step": 5.0,  # Only for exponential = false
        "interval_max": 10.0 * 60,  # Max interval 10 min
        "jitter": False  # If true, random seconds between 0 and exponential calculated time
    }}
}

# Default delay to start processing an alert. Only for alert generation.
# Delay is calculated from alert create_time, not from the moment the alert was received.
# Minimum delay will be 10 secs.
# Alert recovery has a delay of 10 seconds.
# Delay calculated following these options in order:
# - alert.attributes['eventTags']['START_ACTION_DELAY_SECONDS']
# - alert.attributes['eventTags']['ACTION_DELAY']
# - alert.attributes['actionDelay']
# - config <ALERTER_NAME>_CONFIG['actionDelay']
# - config <ALERTER_NAME>_ACTION_DELAY
# - config ALERTERS_DEFAULT_ACTION_DELAY
# - Default value: 180.0
# Actual delay will be from -5 to +5 secs from the given delay.
ALERTERS_DEFAULT_ACTION_DELAY = 60.0

#
# QUEUE CONFIGURATION (NEED TO BE AT THE END TO READ EVERY QUEUE CONFIGURED IN ALL ALERTERS
#
_all_queues = set()
_all_queues.add(globals().get('CELERY_DEFAULT_QUEUE', 'alert'))
_all_queues.add(globals().get('AUTO_CLOSE_TASK_QUEUE', 'alert'))
_all_queues.update({v['queue'] for op, v in globals().get('ALERTERS_DEFAULT_TASKS_DEFINITION',
                                                          {}).items() if 'queue' in v})
_alerters_queues = os.getenv('ALERTERS_CELERY_QUEUES')
if _alerters_queues:
    try:
        _alerters_queues = json.loads(_alerters_queues)
    except:  # noqa
        _alerters_queues = [x.strip() for x in _alerters_queues.split(',')]
else:
    _alerters_queues = []

_all_queues.update(_alerters_queues)

#
# CONFIGURATION FOR WORKERS
#
# Default queues a worker will consume.
# Use -Q parameter in worker launch to consume a different list of queues
CELERY_QUEUES = [Queue(q) for q in _all_queues]
