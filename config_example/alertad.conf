import json
import os
from json import JSONDecodeError

from kombu import Queue  # noqa

# DEBUG = True

# DATABASE_URL should be provided as environment variable
# DATABASE_URL = 'iometrics://postgres@127.0.0.1/monitoring?connect_timeout=10&application_name=alerta'
DATABASE_NAME = 'monitoring'

#
# DEDUPLICATION
#
# DEFAULT_DEDUPLICATION_TYPE: both or attribute.
# If 'both', original Alerta deduplication (based on resource and event) is tried before.
# If 'attribute', original deduplication is not tried and deduplication will depend only on attribute 'deduplication'.
# Default: 'both'
# DEFAULT_DEDUPLICATION_TYPE = 'attribute'
#
# DEFAULT_DEDUPLICATION_TEMPLATE
# IF exists, render template to get deduplication value (if no deduplication attribute is received)
# DEFAULT_DEDUPLICATION_TEMPLATE = '{{ alert.attribute.deduplication | default(alert.id) }}'

#
# LOGGING CONFIGURATION
#
LOG_CONFIG_FILE = os.path.join(os.path.dirname(__file__), 'logging.yaml')
# LOG_LEVEL = 'DEBUG'
# LOG_HANDLERS = ['console']
# LOG_FORMAT = 'default'  # default, simple, verbose, json, syslog
# LOG_FORMAT = '%(asctime)s %(name)s[%(process)d]: [%(levelname)s] %(message)s [in %(pathname)s:%(lineno)d]'
# LOG_HANDLERS = ['file']
# LOG_FILE = '/var/log/alertad.log'
# LOG_MAX_BYTES = 5*1024*1024  # 5 MB
# LOG_BACKUP_COUNT = 2

#
# SECURITY
#
AUTH_REQUIRED = True
SECRET_KEY = 'the_secret_key'  # Overwrite with env var
ADMIN_USERS = ["alertaio-admin@datadope.io"]  # Overwrite with env var
API_KEY_EXPIRE_DAYS = 3650
# DEFAULT_ADMIN_ROLE = 'ops'
# ADMIN_ROLES = ['ops', 'devops', 'coolkids']
USER_DEFAULT_SCOPES = ['read']
# CUSTOMER_VIEWS = True

ALLOWED_ENVIRONMENTS = ['prod', '.*']
DEFAULT_ENVIRONMENT = 'prod'

#
# WEB CONFIG
#
# DEFAULT_PAGE_SIZE = 50
# HISTORY_LIMIT = 100
# HISTORY_ON_VALUE_CHANGE = True
# BULK_QUERY_LIMIT = 100000

#
# CELERY CONFIGURATION
#
CELERY_BROKER_URL = os.getenv('CELERY_BROKER_URL', 'redis://localhost:6379/0')
CELERY_RESULT_BACKEND = os.getenv('CELERY_RESULT_BACKEND', CELERY_BROKER_URL)
CELERY_IMPORTS = ("datadope_alerta.bgtasks", )
CELERY_CREATE_MISSING_QUEUES = True
CELERY_DEFAULT_QUEUE = 'alert'
# For alerta tasks. Not consumed by default. Must run a specific worker pointing to specific celery var and this queue
CELERY_ROUTES = {
    "alerta.tasks.*": "web"
}
CELERYD_LOG_FORMAT = "%(asctime)s|%(levelname)s|-|-|-|%(message)s[[%(name)s|%(processName)s]]"  # Add logger name
CELERYD_TASK_LOG_FORMAT = \
    "%(asctime)s|%(levelname)s|%(alert_id)s|%(alerter_name)s|%(operation)s|%(message)s" \
    "[[%(name)s|%(processName)s][%(task_name)s|%(task_id)s]]"  # Add logger name


BROKER_TRANSPORT_OPTIONS = {
    "queue_order_strategy": "priority",
    "sep": ":",  # Default: \x06\x16
    "priority_steps": list(range(10)),  # Default: 4 ranges
}

_broker_use_ssl = os.getenv('BROKER_USE_SSL')
if _broker_use_ssl:
    if isinstance(_broker_use_ssl, str):
        _broker_use_ssl = json.loads(_broker_use_ssl)
    BROKER_USE_SSL = _broker_use_ssl

# Auto close background task configuration
AUTO_CLOSE_TASK_INTERVAL = 60.0
"""
Interval for auto close periodic task. Default 60 sec.
"""

AUTO_CLOSE_TASK_QUEUE = os.getenv('AUTO_CLOSE_TASK_QUEUE', 'autoclose')
AUTO_RESOLVE_TASK_QUEUE = os.getenv('AUTO_RESOLVE_TASK_QUEUE', 'autoresolve')

ASYNC_ALERT_TASK_QUEUE = os.getenv('ASYNC_ALERT_TASK_QUEUE', 'async_alert')

# Recovery actions
RECOVERY_ACTIONS = {
    "taskQueue": 'recovery_actions',
    "statusQueue": 'recovery_actions',
    "waitQueue": 'recovery_actions',
    "maxRetries": 5
}

#
# ALERTACLIENT_CONFIGURATION:
# Configuration for connecting to alerta sever from celery workers.
# Needed for periodic task for closing automatically alerts.
#
ALERTACLIENT_CONFIGURATION = dict(
    endpoint="http://localhost:8000",
    key=None,
    secret=None,
    token=None,
    username=None,
    password=None,
    timeout=10.0,
    ssl_verify=True,
    ssl_cert=None,
    ssl_key=None,
    headers=None,
    debug=False)

#
# HOUSEKEEPING
# Debe ejecutarse el proceso de housekeeping con el cliente (alerta housekeeping)
# o con una conexiÃ³n al servidor GET /management/housekeeping
# El usuario asociado debe tener permiso admin:management
#
# ALERT_TIMEOUT = 86400  # Default alert timeout. Default 24 hours. 0 for no timeout
# ACK_TIMEOUT = 7200  # timeout for unack alerts in ack status (default is 7200 seconds, 0 = do not auto-unack)
# SHELVE_TIMEOUT = 7200  # timeout for unshelving alerts in shelved status (default 7200, 0 = do not auto-unshelve)
DELETE_EXPIRED_AFTER = 0  # Default 7200 (2 hours). 0 for not deleting expired
DELETE_INFO_AFTER = 0  # Default 43200 (12 hours). 0 for not deleting informational
DELETE_CLOSED_AFTER = 0  # Default to DELETE_EXPIRED_AFTER. 0 for not deleting closed -> Only for iometrics backend.

# CONDITION_RESOLVED_ACTION_NAME = "resolve"
# """
# Default: 'resolve'.
# If this action is received, it will be mapped to close if CLOSED_AT_ORIGIN_MUST_CLOSE var is True (default).
# """
CONDITION_RESOLVED_MUST_CLOSE = False

#
# WEB-UI CONFIGURATION
#
ACTIONS = ['resolve']
SITE_LOGO_URL = "logos/IOMETRICS.png"
SIGNUP_ENABLED = False
COLUMNS = [
    "severity",
    "status",
    "createTime",
    "lastReceiveTime",
    # "timeout",
    "duplicateCount",
    "deduplication",
    "environment",
    "service",
    "resource",
    "event",
    "value",
    "tags",
    # "serviceManagerLink",
    "moreInfo",
    "text"
]

#
# PLUGINS CONFIGURATION
#
ROUTING_DIST = 'datadope-alerta'
PLUGINS_RAISE_ON_ERROR = False
_plugins = os.getenv('PLUGINS', [
    "remote_ip",
    # "reject",
    # "heartbeat",
    # "forwarder",
    "blackout_manager",
    "iom_preprocess",
    "notifier",
    "recovery_actions",
    "zabbix_base"
])
if isinstance(_plugins, str):
    try:
        _plugins = json.loads(_plugins)
    except JSONDecodeError:
        _plugins = [x.strip() for x in _plugins.split(',')]
PLUGINS = _plugins

_alerters = os.getenv('ALERTER_PLUGINS', ['email'])
if isinstance(_alerters, str):
    try:
        _alerters = json.loads(_alerters)
    except JSONDecodeError:
        _alerters = [x.strip() for x in _alerters.split(',')]
PLUGINS.extend([x for x in _alerters if x not in PLUGINS])

#
# BLACKOUTS PLUGIN_CONFIGURATION
#
BLACKOUT_DURATION = 7200  # 2 hours
NOTIFICATION_BLACKOUT = True
BLACKOUT_ACCEPT = ['normal', 'ok', 'cleared']
BLACKOUT_PROVIDERS = ['internal']
BLACKOUT_TASK_INTERVAL = 300
BLACKOUT_TASK_QUEUE = 'blackouts'

# Location of Alerters templates dir
# Default in code: templates dir in datadope-alerta project.
# Modified to use templates dir as sibling os this config file dir
ALERTERS_TEMPLATES_LOCATION = os.path.abspath(os.path.join(os.path.dirname(__file__), '../templates'))

#
# Operations to configure: new and recovery.
# Every alerter may override this config using configuration <ALERTER_NAME>_TASKS_DEFINITION.
# Operations configuration can be received as an alert attribute (tasksDefinition)
# or an alert eventTag (TASKS_DEFINITION).
# Value must be a dictionary (o string with json dictionary format) with these keys:
#     - queue (str): Name of the queue that will process the operation
#     - priority (int): Priority for the task (0 highest priority, 9 lowest priority)
#     - retry_spec (dict): Specification of the mechanism for retries
#         - max_retries (int): Maximum number of retries
#         - exponential (bool): If False, interval between two retries is calculated exponentially
#         - interval_first (float): First interval
#         - interval_step (float): Additional interval between two retries (only used if exponential is False)
#         - interval_max (float): Maximum interval between two retries
#         - jitter (bool): If true, retry will be executed in a random instant between 0 and the calculated interval
#
# Interval calculation:
#     - If exponential is True: interval = min(interval_max, interval_first * (2 ** <retries>))
#     - If exponential is False: interval = min(interval_max, interval_first + interval_step * <retries>)
#
#     In both cases, if jitter is True, the actual interval will be a random instant between 0
#     and the interval calculated with the previous formula.
ALERTERS_DEFAULT_TASKS_DEFINITION = {
    "new": {"queue": "alert", "priority": 1, "retry_spec": {
        "max_retries": 32,
        "exponential": True,
        "interval_first": 2.0,  # First retry after 2 secs
        "interval_step": 5.0,  # Only for exponential = false
        "interval_max": 10.0 * 60,  # Max interval 10 min
        "jitter": False  # If true, random seconds between 0 and exponential calculated time
    }},
    "recovery": {"queue": "recovery", "priority": 6, "retry_spec": {
        "max_retries": 16,
        "exponential": True,
        "interval_first": 2.0,  # First retry after 2 secs
        "interval_step": 5.0,  # Only for exponential = false
        "interval_max": 10.0 * 60,  # Max interval 10 min
        "jitter": False  # If true, random seconds between 0 and exponential calculated time
    }},
    "repeat": {"queue": "repeat", "priority": 7, "retry_spec": {
        "max_retries": 5,
        "exponential": True,
        "interval_first": 2.0,  # First retry after 2 secs
        "interval_step": 5.0,  # Only for exponential = false
        "interval_max": 10.0 * 60,  # Max interval 10 min
        "jitter": False  # If true, random seconds between 0 and exponential calculated time
    }},
    "action": {"queue": "action", "priority": 6, "retry_spec": {
        "max_retries": 5,
        "exponential": True,
        "interval_first": 2.0,  # First retry after 2 secs
        "interval_step": 5.0,  # Only for exponential = false
        "interval_max": 10.0 * 60,  # Max interval 10 min
        "jitter": False  # If true, random seconds between 0 and exponential calculated time
    }}
}

# Default delay to start processing an alert. Only for alert generation.
# Delay is calculated from alert create_time, not from the moment the alert was received.
# Minimum delay will be 5 secs.
# Alert recovery has a delay of 10 seconds.
# Delay calculated following these options in order:
# - alert.attributes['eventTags']['START_ACTION_DELAY_SECONDS']
# - alert.attributes['eventTags']['ACTION_DELAY']
# - alert.attributes['actionDelay']
# - config <ALERTER_NAME>_CONFIG['actionDelay']
# - config <ALERTER_NAME>_ACTION_DELAY
# - config ALERTERS_DEFAULT_ACTION_DELAY
# - Default value: 180.0
# Actual delay will be from -2 to +5 secs from the given delay.
ALERTERS_DEFAULT_ACTION_DELAY = 60.0

#
# CONFIGURATION FOR WORKERS
#

#
# QUEUE CONFIGURATION (NEED TO BE AT THE END TO READ EVERY QUEUE CONFIGURED IN ALL ALERTERS
#
_specific_queues = os.getenv('CELERY_QUEUES')
if _specific_queues:
    try:
        _specific_queues = json.loads(_specific_queues)
    except:  # noqa
        _specific_queues = [x.strip() for x in _specific_queues.split(',')]
if _specific_queues:
    _all_queues = set(_specific_queues)
else:
    # Try to obtain all configured queues.
    # Extra queues may be included using env vars:
    #   * ALERTERS_CELERY_QUEUES
    #   * EXTRA_CELERY_QUEUES
    # Some calculated queue may be ignored using env var 'IGNORE_CELERY_QUEUES'
    _all_queues = set()
    _all_queues.add(globals().get('CELERY_DEFAULT_QUEUE', 'alert'))
    _all_queues.add(globals().get('AUTO_CLOSE_TASK_QUEUE', 'alert'))
    _all_queues.add(globals().get('AUTO_RESOLVE_TASK_QUEUE', 'alert'))
    _all_queues.add(globals().get('ASYNC_ALERT_TASK_QUEUE', 'alert'))
    _all_queues.add(globals().get('RECOVERY_ACTIONS', {}).get('taskQueue', 'alert'))
    _all_queues.add(globals().get('RECOVERY_ACTIONS', {}).get('statusQueue', 'alert'))
    _all_queues.add(globals().get('RECOVERY_ACTIONS', {}).get('waitQueue', 'alert'))
    _all_queues.add(globals().get('BLACKOUT_TASK_QUEUE', 'alert'))
    _all_queues.update({v['queue'] for op, v in globals().get('ALERTERS_DEFAULT_TASKS_DEFINITION',
                                                              {}).items() if 'queue' in v})
    _alerters_queues = os.getenv('ALERTERS_CELERY_QUEUES')
    if _alerters_queues:
        try:
            _alerters_queues = json.loads(_alerters_queues)
        except:  # noqa
            _alerters_queues = [x.strip() for x in _alerters_queues.split(',')]
    else:
        _alerters_queues = []
    _all_queues.update(_alerters_queues)

    _extra_queues = os.getenv('EXTRA_CELERY_QUEUES')
    if _extra_queues:
        try:
            _extra_queues = json.loads(_extra_queues)
        except:  # noqa
            _extra_queues = [x.strip() for x in _alerters_queues.split(',')]
    else:
        _extra_queues = []
    _all_queues.update(_extra_queues)

    _ignore_queues = os.getenv('IGNORE_CELERY_QUEUES')
    if _ignore_queues:
        try:
            _ignore_queues = json.loads(_ignore_queues)
        except:  # noqa
            _ignore_queues = [x.strip() for x in _ignore_queues.split(',')]
    else:
        _ignore_queues = []
    _all_queues = {x for x in _all_queues if x not in _ignore_queues}

# Default queues a worker will consume.
# Use -Q parameter in worker launch to consume a different list of queues
CELERY_QUEUES = [Queue(q) for q in _all_queues]

if 'LDAP_URL' in os.environ:
    LDAP_URL = os.environ['LDAP_URL']

if 'LDAP_BIND_USERNAME' in os.environ:
    LDAP_BIND_USERNAME = os.environ['LDAP_BIND_USERNAME']

if 'LDAP_BASEDN' in os.environ:
    LDAP_BASEDN = os.environ['LDAP_BASEDN']

if 'LDAP_USER_FILTER' in os.environ:
    LDAP_USER_FILTER = os.environ['LDAP_USER_FILTER']

if 'LDAP_DEFAULT_DOMAIN' in os.environ:
    LDAP_DEFAULT_DOMAIN = os.environ['LDAP_DEFAULT_DOMAIN']

if 'LDAP_ALLOW_SELF_SIGNED_CERT' in os.environ:
    LDAP_ALLOW_SELF_SIGNED_CERT = os.environ['LDAP_ALLOW_SELF_SIGNED_CERT'].lower() in ('true', 'yes')

if 'LDAP_TIMEOUT' in os.environ:
    LDAP_TIMEOUT = float(os.environ['LDAP_TIMEOUT'])

LDAP_CONFIG = {'OPT_REFERRALS': 0}

ZABBIX_CONFIG = {
    "platform_field": "origin",
    "supported_platforms": "zabbix",
    "zabbix_reference_attributes": [
        "zabbixEventId",
        "eventId"
    ]
}
